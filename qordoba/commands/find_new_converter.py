import pandas as pd
import yaml
from qordoba.settings import get_localization_files
from qordoba.commands.i18n_base import BaseClass, FilesNotFound
import logging
import nltk
nltk.download('punkt')

log = logging.getLogger('qordoba')

'''
to do: 
add csv, yaml to test
add localization filename column
'''

class FindNewConverter(BaseClass):

    def get_existing_i18n_key_values(self, localization_files):
        keys_values_localisation_files = dict()

        # greps localization file from config and stores all the key value pairs within existing_i18n_KEY_VALUES
        if localization_files:
            import glob
            localization_files = glob.glob(localization_files + "/*")
        else:
            localization_files = get_localization_files()

        for file in localization_files:
            dictionary = self.get_nested_dictionary(file)
            key_values = self.get_all_keys(dictionary, list(), dict())
            keys_values_localisation_files[file] = key_values
        return self.convert(keys_values_localisation_files)

    def index_lookup(self, stringLiteral, localization_k_v):
        # checks if stringLiteral exists in values, gives back corresponding key or None


        for i18n_file in localization_k_v:
            for key, value in localization_k_v[i18n_file].items():
                # print(value, stringLiteral)
                # print(key, stringLiteral)
                if value.strip() == stringLiteral.strip():
                    return key
                if key.strip() == stringLiteral.strip():
                    return key
        return None

    def generate_CSV_with_key_column(self, filepath, df):
        # adding key column to Dataframe with existing keys
        log.info("StringLiterals  were matched to existing keys. Generating new CSV ")
        beginning = '/'.join(filepath.split('/')[:-1])
        filename = filepath.split('/')[-1]
        filename_new = '/key-' + filename

        csv_file = beginning + filename_new
        df.to_csv(csv_file, index=False, encoding='utf-8')

        return df

    def generate_new_keys(self, stringLiteral):
        '''
        For now keys will be generated by taking the 2 words from the String
        which have the least frequency count within the Open National Corpus (spoken & written)
        source http://www.anc.org/data/anc-second-release/frequency-data/
        '''

        ANC_csv = '../../resources/ANC-all-count.csv'
        column_names = ['word', 'again_wtf', 'type', 'count']
        df_ANC = pd.read_csv(ANC_csv, header=None, names=column_names)

        stringLiteral = stringLiteral.replace('-', ' ')
        stringLiteral = stringLiteral.replace('.', ' ')
        stringLiteral = stringLiteral.replace('/', ' ')
        stringLiteral = stringLiteral.replace('_', ' ')
        words = nltk.word_tokenize(stringLiteral)
        words = [word.lower() for word in words if word.isalpha()]

        # take the longest word
        if len(words) == 1:
            return words[0]
        elif len(set(words).intersection(df_ANC.word)) <= 1:
            return (sorted(words,  key=len)[-2:])
        else:
            w1 = None
            w2 = None
            v1 = 0
            v2 = 0
            for w in words:
                # the words with the lowest count
                a = df_ANC.loc[df_ANC['word'] == w]
                s = sum((a['count'].values))
                if v2 == 0 and v1 == 0:
                    v1 = s
                    v2 = s
                    w1 = w
                    w2 = w
                elif s < v1:
                    v1=s
                    w1=w
                elif s < v2 < v1:
                    v2=s
                    w2=w
                else:
                    continue
            return '.'.join(set(([w1,w2])))



    def main(self, filepath, config):
        # loading CSV into dataframe
        column_names = ['filename', 'startLineNumber', 'startCharIdx', 'endLineNumber', 'endCharIdx', 'text']
        df = pd.read_csv(filepath, header=None, names=column_names)

        # getting existing key value pairs from localization files
        localization_k_v = self.get_existing_i18n_key_values(config)

        # converting StringLiterals to pure strings
        df['text'] = df['text'].apply(lambda x: x.replace('"', ''))
        df['text'] = df['text'].apply(lambda x: x.replace("'", ''))

        # lookup if Stringliteral exists as value or key in localization file. If yes, return key, otherwise None.
        df['key'] = df['text'].apply(lambda x: self.index_lookup(x, localization_k_v))

        new_df = self.generate_CSV_with_key_column(filepath, df)
        new_df['new_key'] = new_df['text'].apply(lambda x: self.generate_new_keys(x))

        print(new_df.new_key)


